{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuA6V-btPLp2",
        "outputId": "2b29430c-a715-44ab-af06-55da91d4df89"
      },
      "source": [
        "\n",
        "#Mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HSR6-BmDWB3"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZw2-Zh0DoDm",
        "outputId": "a110f74a-7338-4c04-b7a4-8b283ab1e861"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ccmO90EFFs"
      },
      "source": [
        "BASE_URL = \"https://www.sec.gov/Archives/\"\n",
        "\n",
        "# SEC requires this - read the \"Fair access\" section at https://www.sec.gov/os/accessing-edgar-data\n",
        "# The name and email can be dummy\n",
        "USER_AGENT = \"Nisha nisha@gmail.com\" \n",
        "headers = {\"User-Agent\": USER_AGENT}\n",
        "\n",
        "DOWNLOAD_DELAY = 0.2 # SEC allows max 10 requests per second, hence this should be greater than 1/10 = 0.1s\n",
        "\n",
        "DATA_DIR = \"drive/MyDrive/Blackcoffer\" # Path to directory containing excel files\n",
        "\n",
        "CIK_DATA_PATH = os.path.join(DATA_DIR,\"cik_list.xlsx\")\n",
        "\n",
        "SEC_DATA_DIR = os.path.join(DATA_DIR,\"sec_data\") # Where to download txt files\n",
        "\n",
        "OUTPUT_FILE_PATH = os.path.join(DATA_DIR,\"output.xlsx\")\n",
        "\n",
        "\n",
        "MASTER_DICTIONARY_PATH = os.path.join(DATA_DIR,\"LoughranMcDonald_MasterDictionary_2020.xlsx\")\n",
        "\n",
        "STOPWORDS_GENERIC_PATH      = os.path.join(DATA_DIR,\"StopWords_Generic.txt\")\n",
        "STOPWORDS_GENERIC_LONG_PATH = os.path.join(DATA_DIR,\"StopWords_GenericLong.txt\")\n",
        "\n",
        "CONSTRAINING_DICTIONARY_PATH = os.path.join(DATA_DIR,\"constraining_dictionary.xlsx\")\n",
        "UNCERTAINITY_DICTIONARY_PATH = os.path.join(DATA_DIR,\"uncertainty_dictionary.xlsx\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "155zUGSaFAXl"
      },
      "source": [
        "master_dictionary = pd.read_excel(MASTER_DICTIONARY_PATH)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "ab1qMkydFPKN",
        "outputId": "21fb5e72-b91d-4d0e-ad56-600a94ac2ac7"
      },
      "source": [
        "master_dictionary.sample(3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Seq_num</th>\n",
              "      <th>Word Count</th>\n",
              "      <th>Word Proportion</th>\n",
              "      <th>Average Proportion</th>\n",
              "      <th>Std Dev</th>\n",
              "      <th>Doc Count</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Uncertainty</th>\n",
              "      <th>Litigious</th>\n",
              "      <th>Strong_Modal</th>\n",
              "      <th>Weak_Modal</th>\n",
              "      <th>Constraining</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Syllables</th>\n",
              "      <th>Source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8940</th>\n",
              "      <td>BROKENNESSES</td>\n",
              "      <td>8941</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52792</th>\n",
              "      <td>OVERPARTICULAR</td>\n",
              "      <td>52796</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14583</th>\n",
              "      <td>CONCEITEDLY</td>\n",
              "      <td>14584</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12of12inf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Word  Seq_num  Word Count  ...  Complexity  Syllables     Source\n",
              "8940     BROKENNESSES     8941           0  ...           0          4  12of12inf\n",
              "52792  OVERPARTICULAR    52796           0  ...           0          6  12of12inf\n",
              "14583     CONCEITEDLY    14584           0  ...           0          4  12of12inf\n",
              "\n",
              "[3 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzwfNIzmZ3ra"
      },
      "source": [
        "positive_words = master_dictionary[master_dictionary[\"Positive\"]>0][\"Word\"]\n",
        "positive_words = positive_words.apply(lambda x:str(x).lower()).tolist()\n",
        "positive_words = set(positive_words) # Sets are better than lists for searching\n",
        "\n",
        " \n",
        "negative_words = master_dictionary[master_dictionary[\"Negative\"]>0][\"Word\"]\n",
        "negative_words = negative_words.apply(lambda x:str(x).lower()).tolist()\n",
        "negative_words = set(negative_words)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukpeTg0baZTv"
      },
      "source": [
        "constraining_words = set(pd.read_excel(CONSTRAINING_DICTIONARY_PATH)[\"Word\"].tolist())\n",
        "uncertainity_words = set(pd.read_excel(UNCERTAINITY_DICTIONARY_PATH)[\"Word\"].tolist())"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVwmcNO8apSK"
      },
      "source": [
        "with open(STOPWORDS_GENERIC_PATH,'r') as f:\n",
        "  generic_stopwords = set(f.readlines())\n",
        "\n",
        "with open(STOPWORDS_GENERIC_LONG_PATH,'r') as f:\n",
        "  long_stopwords = set(f.readlines())\n",
        "\n",
        "nltk_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "all_stopwords = nltk_stopwords | generic_stopwords | long_stopwords # Take union of all sets"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR8RnLB5avIm",
        "outputId": "8c04d82d-cb7a-4f5c-81b8-151042802e41"
      },
      "source": [
        "print(f\"No. of positive words - {len(positive_words)}\")\n",
        "\n",
        "print(f\"No. of negative words - {len(negative_words)}\")\n",
        "\n",
        "print(f\"No. of generic stopwords - {len(generic_stopwords)}\")\n",
        "\n",
        "print(f\"No. of long stopwords - {len(long_stopwords)}\")\n",
        "\n",
        "print(f\"No. of nltk stopwords - {len(nltk_stopwords)}\")\n",
        "\n",
        "print(f\"No. of all stopwords - {len(all_stopwords)}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of positive words - 347\n",
            "No. of negative words - 2345\n",
            "No. of generic stopwords - 121\n",
            "No. of long stopwords - 570\n",
            "No. of nltk stopwords - 179\n",
            "No. of all stopwords - 870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYzzHgVMa0fr"
      },
      "source": [
        "cik_data = pd.read_excel(CIK_DATA_PATH)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "fop8mnSzbDlc",
        "outputId": "856ae5ba-c786-4969-b399-42e7968f81a9"
      },
      "source": [
        "cik_data.sample(3)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CIK</th>\n",
              "      <th>CONAME</th>\n",
              "      <th>FYRMO</th>\n",
              "      <th>FDATE</th>\n",
              "      <th>FORM</th>\n",
              "      <th>SECFNAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3982</td>\n",
              "      <td>ALLIS CHALMERS ENERGY INC.</td>\n",
              "      <td>200611</td>\n",
              "      <td>2006-11-08</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/3982/0000950129-06-009522.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199906</td>\n",
              "      <td>1999-06-11</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/3662/0000950170-99-001005.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>5588</td>\n",
              "      <td>AMERICAN PAD &amp; PAPER CO</td>\n",
              "      <td>199708</td>\n",
              "      <td>1997-08-14</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/5588/0000950134-97-006223.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     CIK  ...                                  SECFNAME\n",
              "25  3982  ...  edgar/data/3982/0000950129-06-009522.txt\n",
              "11  3662  ...  edgar/data/3662/0000950170-99-001005.txt\n",
              "68  5588  ...  edgar/data/5588/0000950134-97-006223.txt\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "MXz3Vme8bJGn",
        "outputId": "f2310c34-2755-47ff-8f20-7ad65f220a18"
      },
      "source": [
        "cik_data.sample(3)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CIK</th>\n",
              "      <th>CONAME</th>\n",
              "      <th>FYRMO</th>\n",
              "      <th>FDATE</th>\n",
              "      <th>FORM</th>\n",
              "      <th>SECFNAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>4515</td>\n",
              "      <td>AMERICAN AIRLINES INC</td>\n",
              "      <td>200810</td>\n",
              "      <td>2008-10-16</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/4515/0000004515-08-000073.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>5907</td>\n",
              "      <td>AT&amp;T CORP</td>\n",
              "      <td>200011</td>\n",
              "      <td>2000-11-14</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/5907/0000005907-00-000038.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199812</td>\n",
              "      <td>1998-12-22</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/3662/0000950170-98-002402.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     CIK                 CONAME  ...  FORM                                  SECFNAME\n",
              "53  4515  AMERICAN AIRLINES INC  ...  10-Q  edgar/data/4515/0000004515-08-000073.txt\n",
              "80  5907              AT&T CORP  ...  10-Q  edgar/data/5907/0000005907-00-000038.txt\n",
              "7   3662       SUNBEAM CORP/FL/  ...  10-Q  edgar/data/3662/0000950170-98-002402.txt\n",
              "\n",
              "[3 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLMI_Gy3bO1k"
      },
      "source": [
        "download_links = cik_data[\"SECFNAME\"].apply(lambda link: BASE_URL+link)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM7_5UjsbRRD",
        "outputId": "f2a264db-fd08-444b-8d2e-b76c8948741a"
      },
      "source": [
        "print(f\"No. of files to download - {len(download_links)}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of files to download - 152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Deui99bax2"
      },
      "source": [
        "def download_sec_data(links_to_download,download_dir):\n",
        "  \"\"\"\n",
        "  Downloads SEC data.\n",
        "  Stops on the first failure\n",
        "\n",
        "\n",
        "  Params:\n",
        "    links_to_download([str]) - List of links to download\n",
        "    download_dir(str) - Directory where to download files\n",
        "\n",
        "  Returns:\n",
        "    status(int) - 0(success), 1(failure)\n",
        "  \"\"\"\n",
        "  if not os.path.exists(download_dir):\n",
        "    os.mkdir(download_dir)\n",
        "    print(f\"Created directory {download_dir}\")\n",
        "\n",
        "  try:\n",
        "    with requests.Session() as session:\n",
        "      for i,link in enumerate(tqdm(links_to_download)):\n",
        "        response = session.get(link,headers=headers)\n",
        "\n",
        "        FILE_NAME = os.path.join(download_dir,f\"{i}.txt\")\n",
        "\n",
        "        with open(FILE_NAME,\"w\") as f:\n",
        "          f.write(response.text)\n",
        "        \n",
        "        time.sleep(DOWNLOAD_DELAY) # Add delay in between requests so SEC doesn't block our IP\n",
        "  except Exception as e:\n",
        "    print(f\"Download failed\")\n",
        "    print(e)\n",
        "    return 1\n",
        "\n",
        "  print(\"Download successful\")\n",
        "\n",
        "  return 0"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3aJhJUKibIM",
        "outputId": "7fed538f-0b34-4796-b4c6-899e4ebbb4b3"
      },
      "source": [
        "%%time\n",
        "#download_sec_data(download_links,SEC_DATA_DIR)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/152 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Created directory drive/MyDrive/Blackcoffer/sec_data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 152/152 [00:43<00:00,  3.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Download successful\n",
            "CPU times: user 1.42 s, sys: 265 ms, total: 1.68 s\n",
            "Wall time: 43.8 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Pb-ZJgjV-t"
      },
      "source": [
        "### Calculating metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPtqpLNQi5BX"
      },
      "source": [
        "def get_num_of_syllables(word):\n",
        "  word = word.lower()\n",
        "  vowels = ['a','e','i','o','u']\n",
        "\n",
        "  syllables = 0\n",
        "\n",
        "  for char in word:\n",
        "    if char in vowels:\n",
        "      syllables += 1\n",
        "\n",
        "  return syllables\n",
        "\n",
        "def get_scores(words):\n",
        "  \"\"\"\n",
        "  Calculate various scores such as positive_score,constraining_score\n",
        "\n",
        "  Params:\n",
        "    words([str]) - List of words\n",
        "\n",
        "  Returns:\n",
        "    result((int)) - (positive_score,negative_score,\n",
        "          uncertainty_score,constraining_score,complex_word_count)\n",
        "  \"\"\"\n",
        "  positive_score = 0\n",
        "  negative_score = 0\n",
        "  uncertainty_score  = 0\n",
        "  constraining_score = 0\n",
        "  complex_word_count = 0\n",
        "\n",
        "  for word in words:\n",
        "    num_of_syllables = get_num_of_syllables(word)\n",
        "    if num_of_syllables > 2:\n",
        "      complex_word_count += 1\n",
        "    if word in positive_words:\n",
        "      positive_score += 1\n",
        "    if word in negative_words:\n",
        "      negative_score += 1\n",
        "    if word in uncertainity_words:\n",
        "      uncertainty_score += 1\n",
        "    if word in constraining_words:\n",
        "      constraining_score += 1\n",
        "\n",
        "  return (positive_score,negative_score,\n",
        "          uncertainty_score,constraining_score,complex_word_count)\n",
        "\n",
        "def get_polarity_score(positive_score,negative_score):\n",
        "  \"\"\"\n",
        "  This is the score that determines if a given text is positive or negative in nature. \n",
        "  It is calculated by using the formula:\n",
        "  Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
        "  Range is from -1 to +1    \n",
        "  \"\"\"\n",
        "  return (positive_score-negative_score)/(positive_score+negative_score+0.000001)\n",
        "\n",
        "def get_subjectivity_score(positive_score,negative_score,num_of_words_after_cleaning):\n",
        "  \"\"\"\n",
        "  This is the score that determines if a given text is objective or subjective.\n",
        "  It is calculated by using the formula:\n",
        "  Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
        "  Range is from 0 to +1\n",
        "  \"\"\"\n",
        "  return (positive_score+negative_score)/(num_of_words_after_cleaning+0.000001)\n",
        "\n",
        "\n",
        "def get_sentiment_category(polarity):\n",
        "  if polarity < -0.5:\n",
        "    return \"Most Negative\"\n",
        "  elif -0.5 < polarity < 0:\n",
        "    return \"Negative\"\n",
        "  elif polarity == 0.0:\n",
        "    return \"Neutral\"\n",
        "  elif 0 < polarity < 0.5:\n",
        "    return \"Positive\"\n",
        "  else:\n",
        "    return \"Very Positive\"\n",
        "\n",
        "\n",
        "def get_average_sentence_length(num_of_words,num_of_sentences):\n",
        "  \"Average Sentence Length = the number of words / the number of sentences\"\n",
        "  return (num_of_words/(num_of_sentences+1)) # +1 to avoid Division by 0\n",
        "\n",
        "def get_percentage_of_complex_words(num_of_complex_words,num_of_words):\n",
        "  \"Percentage of Complex words = the number of complex words / the number of words\"\n",
        "  return (num_of_complex_words/(num_of_words+1)) # +1 to avoid Division by 0\n",
        "\n",
        "def get_fog_index(average_sentence_length,percentage_of_complex_words):\n",
        "  \"Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\"\n",
        "  return (0.4 * (average_sentence_length + percentage_of_complex_words))\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am_gVagckOHm"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMX5SdtHkGq4"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "  return (\"\".join([char for char in text if char not in string.punctuation]))\n",
        "\n",
        "def remove_stop_words(words,stopwords):\n",
        "  words = [word for word in words if word not in stopwords]\n",
        "\n",
        "  return words\n",
        "\n",
        "def lemmatize(words):\n",
        "    word_net = nltk.WordNetLemmatizer()\n",
        "    return [word_net.lemmatize(word) for word in words]\n",
        "\n",
        "def remove_useless_text(text):\n",
        "  \"\"\"\n",
        "  Remove useless text using heuristics\n",
        "  \"\"\"\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "def remove_html(text):\n",
        "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "  return text\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  # Remove html tags\n",
        "  text = remove_html(text)\n",
        "\n",
        "  text = remove_punctuation(text)\n",
        "\n",
        "  words = nltk.word_tokenize(text)\n",
        "\n",
        "  words = lemmatize(words)\n",
        "\n",
        "  words = remove_stop_words(words,all_stopwords) # Needs clarification, which set of stopwords to use?\n",
        "\n",
        "  return words"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4vkXTSukbl5"
      },
      "source": [
        "def analyze(data_dir):\n",
        "  \"\"\"\n",
        "  Perform the whole analysis\n",
        "  \"\"\"\n",
        "  for index,file_name in enumerate(tqdm(os.listdir(data_dir))):\n",
        "    file_path = os.path.join(data_dir,file_name)\n",
        "\n",
        "    with open(file_path,'r') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = preprocess(text)\n",
        "\n",
        "    num_of_words = len(words)\n",
        "    num_of_sentences = len(sentences)\n",
        "\n",
        "    positive_score,negative_score,uncertainty_score,constraining_score,complex_word_count = get_scores(words)\n",
        "\n",
        "    polarity_score = get_polarity_score(positive_score,negative_score)\n",
        "\n",
        "    average_sentence_length = get_average_sentence_length(num_of_words,num_of_sentences)\n",
        "\n",
        "    percentage_of_complex_words = complex_word_count/(num_of_words+1)\n",
        "\n",
        "    positive_word_proportion = positive_score/(num_of_words+1)\n",
        "    negative_word_proportion = negative_score/(num_of_words+1)\n",
        "\n",
        "    uncertainty_word_proportion = uncertainty_score/(num_of_words+1)\n",
        "    constraining_word_proportion = constraining_score/(num_of_words+1)\n",
        "\n",
        "    fog_index = get_fog_index(average_sentence_length,percentage_of_complex_words)\n",
        "\n",
        "    constraining_words_whole_report = constraining_score # This needs to be clarified and fixed\n",
        "\n",
        "    cik_data.loc[index,\"positive_score\"] = positive_score\n",
        "    cik_data.loc[index,\"negative_score\"] = negative_score\n",
        "    cik_data.loc[index,\"polarity_score\"] = polarity_score\n",
        "    cik_data.loc[index,\"average_sentence_length\"] = average_sentence_length\n",
        "    cik_data.loc[index,\"percentage_of_complex_words\"] = percentage_of_complex_words\n",
        "    cik_data.loc[index,\"fog_index\"] = fog_index\n",
        "    cik_data.loc[index,\"complex_word_count\"] = complex_word_count\n",
        "    cik_data.loc[index,\"word_count\"] = num_of_words\n",
        "    cik_data.loc[index,\"uncertainty_score\"] = uncertainty_score\n",
        "    cik_data.loc[index,\"constraining_score\"] = constraining_score\n",
        "    cik_data.loc[index,\"positive_word_proportion\"] = positive_word_proportion\n",
        "    cik_data.loc[index,\"negative_word_proportion\"] = negative_word_proportion\n",
        "    cik_data.loc[index,\"uncertainty_word_proportion\"] = uncertainty_word_proportion\n",
        "    cik_data.loc[index,\"constraining_word_proportion\"] = constraining_word_proportion\n",
        "    cik_data.loc[index,\"constraining_words_whole_report\"] = constraining_words_whole_report"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqHFZV5x3iVL",
        "outputId": "0463c691-6e36-46f8-e2e2-bcf2ea86d515"
      },
      "source": [
        "%%time\n",
        "analyze(SEC_DATA_DIR)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 152/152 [02:43<00:00,  1.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 42s, sys: 837 ms, total: 2min 42s\n",
            "Wall time: 2min 43s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBQHuTcn3wrw"
      },
      "source": [
        "cik_data.to_excel(OUTPUT_FILE_PATH)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F05jW8iV5Lku"
      },
      "source": [
        "### Test preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m1OBqjs41s1"
      },
      "source": [
        "sample_file = os.path.join(SEC_DATA_DIR,\"0.txt\")\n",
        "\n",
        "with open(sample_file) as f:\n",
        "  sample_text = f.read()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec-mxFMf5Q75",
        "outputId": "07d83b81-1ea0-459f-8dbe-9f5546f41686"
      },
      "source": [
        "%%time\n",
        "processed_sample_text = preprocess(sample_text)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.48 s, sys: 7.35 ms, total: 1.49 s\n",
            "Wall time: 1.51 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyo_YdPg5VMb",
        "outputId": "cae1fae5-4d4e-4958-df13-f7f3c5397030"
      },
      "source": [
        "print(processed_sample_text[:20])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['begin', 'privacyenhanced', 'message', 'proctype', '2001micclear', 'originatorname', 'webmasterwwwsecgov', 'originatorkeyasymmetric', 'mfgwcgyevqgbaqicaf8dsgawrwjaw2snkk9avtbzyzmr6agjlwyk3xmzv3dtinen', 'twsm7vrzladbmyqaionwg5sdw3p6oam5d3tdezxmm7z1tbtwidaqab', 'micinfo', 'rsamd5rsa', 'evpdkfnjzbijwkek2rgnck152qxomhpnldwlxttxbuazk70ayyrsxlqbyiqr', 'v5559qrytgpe9pfvt0db9q', '000095017098000413txt', '19980309', '000095017098000413hdrsgml', '19980309', 'accession', 'number']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUykh7aL5cMg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}